{"cells":[{"cell_type":"markdown","metadata":{"id":"n-68F9xXjsrl"},"source":["# Config & Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1E9hmiwqL3M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753522980414,"user_tz":-180,"elapsed":5510,"user":{"displayName":"Владимир Бурцев","userId":"05185084735884685100"}},"outputId":"c9e6a960-77de-47ca-87c4-15641771482c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n","Collecting fsspec\n","  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["!pip install -U datasets huggingface_hub fsspec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2nIflq1EUYyO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753522984923,"user_tz":-180,"elapsed":4498,"user":{"displayName":"Владимир Бурцев","userId":"05185084735884685100"}},"outputId":"dd45ddb6-4550-43ff-f9e2-116e5c20791d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"]}],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XLU9vwatJrW"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.nn.functional as F\n","import torchinfo\n","\n","from tqdm import tqdm\n","import math\n","from datasets import load_dataset\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import altair as alt\n","\n","from tokenizers.models import WordPiece\n","from tokenizers import Tokenizer\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.pre_tokenizers import Whitespace"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-PZJEdbnw5N"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_J5Q9fdmzlv"},"outputs":[],"source":["def get_config():\n","    return {\n","        \"batch_size\": 8,\n","        \"num_epochs\": 20,\n","        \"lr\": 2e-4,\n","        \"seq_len\": 250,\n","        \"d_model\": 768,\n","        \"lang_src\": \"en\",\n","        \"lang_trgt\": \"ru\",\n","        'model_folder': 'weights',\n","        'model_basename': 'tmodel_',\n","        \"preload\": None,\n","        \"tokenizer_file\": \"tokenizer_{0}.json\",\n","        \"experiment_name\": \"runs/tmodel\",\n","        \"checkpoint_every\": 50\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpSZHz_0nPyS"},"outputs":[],"source":["def get_weights_file_path(config, epoch):\n","    model_folder = config['model_folder']\n","    model_basename = config['model_basename']\n","    model_filename = f\"{model_basename}_{epoch}.pt\"\n","    return Path('.') / model_folder / model_filename"]},{"cell_type":"markdown","metadata":{"id":"n7DoHEpQj4bR"},"source":["# Dataset & Tokenization & Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WldxOYpmXCY-"},"outputs":[],"source":["def get_all_sentences(ds, lang):\n","    for item in ds:\n","        yield item['translation'][lang]     # item = {'translation': {'en': 'Hello', 'de': 'Hallo'}}, if we have lang = 'en' we will get 'Hello'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bechfFKhTXG-"},"outputs":[],"source":["def get_or_build_tokenizer(config, ds, lang):\n","    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n","    if not Path.exists(tokenizer_path):\n","        tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","        tokenizer.pre_tokenizer = Whitespace()\n","        trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2) # trainer of tokenizer with special tokens\n","        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer) # trains tokenizer with trainer using initialized language\n","        tokenizer.save(str(tokenizer_path)) # saves tokenizer\n","    else:\n","        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # gets tokenizer if it exists\n","    return tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWtckTbuz5Ph"},"outputs":[],"source":["class BilingualDataset(Dataset):\n","\n","    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","\n","        self.ds = ds\n","        self.tokenizer_src = tokenizer_src\n","        self.tokenizer_tgt = tokenizer_tgt\n","        self.src_lang = src_lang\n","        self.tgt_lang = tgt_lang\n","\n","        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n","        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n","        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, idx):\n","        src_target_pair = self.ds[idx]\n","        src_text = src_target_pair['translation'][self.src_lang]\n","        tgt_text = src_target_pair['translation'][self.tgt_lang]\n","\n","        # Transform the text into tokens\n","        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n","        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n","\n","        # Add sos, eos and padding to each sentence\n","        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n","        # We will only add <s>, and </s> only on the label\n","        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n","\n","        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n","        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n","            raise ValueError(\"Sentence is too long\")\n","\n","        # Add <s> and </s> token\n","        encoder_input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(enc_input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        # Add only <s> token\n","        decoder_input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(dec_input_tokens, dtype=torch.int64),\n","                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        # Add only </s> token\n","        label = torch.cat(\n","            [\n","                torch.tensor(dec_input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n","            ],\n","            dim=0,\n","        )\n","\n","        # Double check the size of the tensors to make sure they are all seq_len long\n","        assert encoder_input.size(0) == self.seq_len\n","        assert decoder_input.size(0) == self.seq_len\n","        assert label.size(0) == self.seq_len\n","\n","        return {\n","            \"encoder_input\": encoder_input,  # (seq_len)\n","            \"decoder_input\": decoder_input,  # (seq_len)\n","            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n","            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n","            \"label\": label,  # (seq_len)\n","            \"src_text\": src_text,\n","            \"tgt_text\": tgt_text,\n","        }\n","\n","def causal_mask(size):\n","    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n","    return mask == 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mLDeLY4YJi7"},"outputs":[],"source":["def get_ds(config):\n","    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_trgt\"]}', split='train') # loading dataset\n","\n","    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src']) # tokenizer for src_lang\n","    tokenizer_trgt = get_or_build_tokenizer(config, ds_raw, config['lang_trgt']) #ds tokenizer for trgt_lang\n","\n","    # ========== splitting, creating dataset ===========\n","\n","    train_ds_size = int(0.9 * len(ds_raw))\n","    val_ds_size = len(ds_raw) - train_ds_size\n","    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n","\n","    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_trgt, config['lang_src'], config['lang_trgt'], config['seq_len'])\n","    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_trgt, config['lang_src'], config['lang_trgt'], config['seq_len'])\n","\n","    # ==================================================\n","\n","    max_len_src = 0\n","    max_len_trgt = 0\n","\n","    for item in ds_raw: # loop throught all the samples of the dataset to find max length of sequence and initialize seq_len correctly\n","        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids # tokenizing the sentence and getting the idx of the tokens in src_lang\n","        trgt_ids = tokenizer_trgt.encode(item['translation'][config['lang_trgt']]).ids # tokenizing the sentence and getting the idx of the tokens in trgt_lang\n","        max_len_src = max(max_len_src, len(src_ids))\n","        max_len_trgt = max(max_len_trgt, len(trgt_ids))\n","\n","    print(max_len_src)\n","    print(max_len_trgt)\n","\n","    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True) # creating Dataloader\n","    val_loader = DataLoader(val_ds, batch_size=1, shuffle=True) # creating Dataloader\n","\n","    return train_loader, val_loader, tokenizer_src, tokenizer_trgt"]},{"cell_type":"markdown","metadata":{"id":"JmD8rnM3kDeQ"},"source":["# Building a model(Transformer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jJ-7-_btWPM"},"outputs":[],"source":["class InputEmbed(nn.Module):\n","    def __init__(self, d_model, vocab_size):\n","        super().__init__()\n","        self.vocab = vocab_size\n","        self.d_model = d_model\n","        self.embed = torch.nn.Embedding(vocab_size, d_model)\n","\n","    def forward(self, x):\n","        return self.embed(x) * math.sqrt(self.d_model) # (batch_size, seq_len, d_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsXa2TAWuQJt"},"outputs":[],"source":["class PosEncod(nn.Module):\n","    def __init__(self, d_model, seq_len, drop_rate):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","        self.seq_len = seq_len\n","        self.drop_rate = drop_rate\n","        self.dropout = nn.Dropout(p=self.drop_rate)\n","\n","        pos_enc = torch.zeros(seq_len, d_model)\n","        pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pos_enc[:, 0::2] = torch.sin(pos * div_term)\n","        pos_enc[:, 1::2] = torch.cos(pos * div_term)\n","        pe = pos_enc.unsqueeze(0)\n","\n","        self.register_buffer('pos_enc', pe) # doesn't requires any gradient\n","\n","    def forward(self, x):\n","        x = x + (self.pos_enc[:, :x.size(1), :]).requires_grad_(False)\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Be-u9Majyq7w"},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    def __init__(self, d_model, eps=1e-12):\n","        super().__init__()\n","        self.gamma = nn.Parameter(torch.ones(d_model))\n","        self.beta = nn.Parameter(torch.zeros(d_model))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.gamma * (x - mean) / (std + self.eps) + self.beta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4IdLeatzZAT"},"outputs":[],"source":["class FFBlock(nn.Module):\n","    def __init__(self, d_model, d_ff, drop_rate):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","        self.d_ff = d_ff\n","        self.drop_rate = drop_rate\n","\n","        self.linear1 = nn.Linear(d_model, d_ff)\n","        self.dropout = nn.Dropout(p=self.drop_rate)\n","        self.linear2 = nn.Linear(d_ff, d_model)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.linear2(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNOd23pI6t2b"},"outputs":[],"source":["class MultiheadAttention(nn.Module):\n","    def __init__(self, d_model, h, drop_rate):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","        self.h = h\n","        self.d_k = d_model // self.h\n","        self.attn_weights = None\n","\n","        self.dropout = nn.Dropout(p=drop_rate)\n","\n","        # Linear projection for Q, K, V\n","\n","        self.w_q = nn.Linear(d_model, d_model, bias=False)\n","        self.w_k = nn.Linear(d_model, d_model, bias=False)\n","        self.w_v = nn.Linear(d_model, d_model, bias=False)\n","\n","        # # Linear projection for output after heads connection\n","\n","        self.w_o = nn.Linear(d_model, d_model, bias=False)\n","\n","    @staticmethod\n","    def attention(query, key, value, mask=None, dropout=None): # Scalad Dot-Product Attention\n","        d_k = query.size(-1)\n","        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n","\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, -1e9)\n","\n","        scores = scores.softmax(dim=-1) # prediction --> probabilities\n","\n","        if dropout is not None:\n","            scores = dropout(scores)\n","\n","        # print(scores.shape)\n","        # print(value.shape)\n","\n","        return (scores @ value), scores\n","\n","    def forward(self, q, k, v, mask):\n","        batch_size = q.size(0)\n","\n","        query = self.w_q(q) # (Batch, seq_len, d_model)\n","        key = self.w_k(k) # (Batch, seq_len, d_model)\n","        value = self.w_v(v) # (Batch, seq_len, d_model)\n","\n","        # ========== Splitting into heads ===========\n","\n","        Q = query.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n","        K = key.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n","        V = value.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n","\n","        # ===========================================\n","\n","        x, self.attention_scores = MultiheadAttention.attention(query, key, value, mask, self.dropout)\n","\n","        output, attn_weights = self.attention(Q, K, V, mask, self.dropout)\n","        self.attn_weights = attn_weights\n","\n","        out = output.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k) # (B, h, seq_len, d_k) --> (Batch, seq_len, d_model)\n","        out = self.w_o(out)\n","        # print('Attention has passed')\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFDOsOiX_0f5"},"outputs":[],"source":["class ResidualConnection(nn.Module):\n","    def __init__(self, drop_rate, d_model):\n","        super().__init__()\n","\n","        self.drop_rate = drop_rate\n","        self.norm = LayerNorm(d_model)\n","        self.dropout = nn.Dropout(p=self.drop_rate)\n","\n","    def forward(self, x, sublayer):\n","        return x + self.dropout(sublayer(self.norm(x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_XxrAjJELs0"},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","    def __init__(self, self_attention_block: MultiheadAttention, feed_forward: FFBlock, drop_rate, d_model):\n","        super().__init__()\n","        self.self_attention_block = self_attention_block\n","        self.feed_forward = feed_forward\n","        self.residual_connections = nn.ModuleList([ResidualConnection(drop_rate, d_model) for _ in range(2)])\n","\n","    def forward(self, x, src_mask):\n","        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n","        x = self.residual_connections[1](x, self.feed_forward)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmkxkKzBF9FS"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, layers: nn.ModuleList, d_model):\n","        super().__init__()\n","        self.layers = layers\n","        self.norm = LayerNorm(d_model)\n","\n","    def forward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2tVV9PEJ7wu"},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","    def __init__(self, self_attention_block: MultiheadAttention, cross_attention_block: MultiheadAttention, feed_forward: FFBlock, drop_rate, d_model):\n","        super().__init__()\n","\n","        self.self_attention_block = self_attention_block\n","        self.cross_attention_block = cross_attention_block\n","        self.feed_forward = feed_forward\n","        self.residual_connections = nn.ModuleList([ResidualConnection(drop_rate, d_model) for _ in range(3)])\n","\n","    def forward(self, x, encoder_output, src_mask, trgt_mask):\n","        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, trgt_mask))\n","        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n","        x = self.residual_connections[2](x, self.feed_forward)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MX6iJUSKLp1H"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, layers: nn.ModuleList, d_model):\n","        super().__init__()\n","\n","        self.layers = layers\n","        self.norm = LayerNorm(d_model)\n","\n","    def forward(self, x, encoder_output, src_mask, trgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, encoder_output, src_mask, trgt_mask)\n","        return self.norm(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JP-ka2bJMJKw"},"outputs":[],"source":["class Projection(nn.Module):\n","    def __init__(self, d_model, vocab_size):\n","        super().__init__()\n","\n","        self.proj = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x):\n","        return self.proj(x)"]},{"cell_type":"markdown","metadata":{"id":"YElhKcxpkRuO"},"source":["# Putting everything together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15omqY28Mjtw"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbed, trgt_embed: InputEmbed, src_pos: PosEncod, trgt_pos: PosEncod, proj: Projection, d_model):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed\n","        self.trgt_embed = trgt_embed\n","        self.src_pos = src_pos\n","        self.trgt_pos = trgt_pos\n","        self.proj = proj\n","        self.attention_weights_map = []\n","        self.d_model = d_model\n","\n","    def encode(self, src, src_mask):\n","        src = self.src_embed(src)\n","        src = self.src_pos(src)\n","        out = self.encoder(src, src_mask)\n","        # print('encoding has passed')\n","        return out\n","\n","    def decode(self, encoder_output, src_mask, trgt, trgt_mask):\n","        trgt = self.trgt_embed(trgt)\n","        trgt = self.trgt_pos(trgt)\n","        out = self.decoder(trgt, encoder_output, src_mask, trgt_mask)\n","        # print('Decoding has passed')\n","        return out\n","\n","    def projection(self, x):\n","        return self.proj(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wDi-2fui7hl"},"outputs":[],"source":["def build_transformer(src_size, trgt_size, src_seq_len, trgt_seq_len, d_model=768, N=6, h=8, drop_rate=0.3, d_ff=1024):\n","    src_embed = InputEmbed(d_model, src_size) # creating embeddings for src_lang\n","    trgt_embed = InputEmbed(d_model, trgt_size) # creating embedding for trgt_lang\n","\n","    src_pos = PosEncod(d_model, src_seq_len, drop_rate) # adding positional encoding\n","    trgt_pos = PosEncod(d_model, trgt_seq_len, drop_rate) # adding positional encoding\n","\n","    encoder_blocks = []\n","    for _ in range(N):\n","        encoder_self_attention_block = MultiheadAttention(d_model, h, drop_rate)\n","        feed_forward_block = FFBlock(d_model, d_ff, drop_rate)\n","        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, drop_rate, d_model)\n","        encoder_blocks.append(encoder_block)\n","\n","    decoder_blocks = []\n","    for _ in range(N):\n","        decoder_self_attention_block = MultiheadAttention(d_model, h, drop_rate)\n","        decoder_cross_attention_block = MultiheadAttention(d_model, h, drop_rate)\n","        feed_forward_block = FFBlock(d_model, d_ff, drop_rate)\n","        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, drop_rate, d_model)\n","        decoder_blocks.append(decoder_block)\n","\n","    encoder = Encoder(nn.ModuleList(encoder_blocks), d_model)\n","    decoder = Decoder(nn.ModuleList(decoder_blocks), d_model)\n","\n","    proj_layer = Projection(d_model, trgt_size)\n","    transformer = Transformer(encoder, decoder, src_embed, trgt_embed, src_pos, trgt_pos, proj_layer, d_model)\n","\n","    for p in transformer.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","\n","    return transformer"]},{"cell_type":"markdown","metadata":{"id":"U41yYIeWkW_f"},"source":["# Initializing, training and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh_QL4aymJ_L"},"outputs":[],"source":["def get_model(config, vocab_src_len, vocab_trgt_len):\n","    model = build_transformer(vocab_src_len, vocab_trgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OOcQynrbMVV"},"outputs":[],"source":["def greedy_decode(model, source, source_mask, tokenizer_src,  tokenizer_trgt, max_len, device):\n","    sos_idx = tokenizer_trgt.token_to_id('[SOS]')\n","    eos_idx = tokenizer_trgt.token_to_id('[EOS]')\n","\n","    encoder_output = model.encode(source, source_mask)\n","    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n","\n","    while True:\n","        if decoder_input.size(1) == max_len:\n","            break\n","\n","        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n","\n","        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n","\n","        prob = model.proj(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","\n","        decoder_input = torch.cat([\n","            decoder_input,\n","            torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)\n","        ], dim=1)\n","\n","        if next_word == eos_idx:\n","            break\n","\n","    return decoder_input.squeeze(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTdUyWgmaE6o"},"outputs":[],"source":["def run_validation(model, val_ds, tokenizer_src, tokenizer_trgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n","    model.eval()\n","    count = 0\n","\n","    src_texts = []\n","    expected = []\n","    predicted = []\n","\n","    console_width = 80\n","\n","    with torch.inference_mode():\n","        for batch in val_ds:\n","            count += 1\n","            encoder_input = batch['encoder_input'].to(device)\n","            encoder_mask = batch['encoder_mask'].to(device)\n","\n","            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n","\n","            model_output = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_trgt, max_len, device)\n","\n","            source_text = batch['src_text'][0]\n","            target_text = batch['trgt_text'][0]\n","            model_output_text = tokenizer_trgt.decode(model_output.detach().cpu().numpy())\n","\n","            src_texts.append(source_text)\n","            expected.append(target_text)\n","            predicted.append(model_output_text)\n","\n","            print_msg('-'*console_width)\n","            print_msg(f'SOURCE: {source_text}')\n","            print_msg(f'TARGET: {target_text}')\n","            print_msg(f'PREDICTED: {model_output_text}')\n","\n","            if count > num_examples:\n","                break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRYP_x3_n_NW"},"outputs":[],"source":["def train_model(device, config):\n","    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n","\n","    train_loader, val_loader, tokenizer_src, tokenizer_trgt = get_ds(config)\n","    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_trgt.get_vocab_size()).to(device)\n","\n","    writer = SummaryWriter(config['experiment_name'])\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], eps=1e-9, betas=(0.9, 0.999), weight_decay=0.01)\n","\n","    initial_epoch = 0\n","    global_step = 0\n","    if config['preload']:\n","        model_filename = get_weights_file_path(config, config[\"preload\"])\n","        print(f'Preloading model {model_filename}')\n","        state = torch.load(model_filename)\n","        optimizer.load_state_dict(state['optimizer_state_dict'])\n","        global_step = state['global_step']\n","\n","    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_trgt.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n","\n","    for epoch in range(initial_epoch, config['num_epochs']):\n","        model.train()\n","        batch_iterator = tqdm(train_loader, desc=f'Processing epoch {epoch:02d}')\n","        for batch_idx, batch in enumerate(batch_iterator):\n","            model.train()\n","            encoder_input = batch['encoder_input'].to(device)\n","            decoder_input = batch['decoder_input'].to(device)\n","            encoder_mask = batch['encoder_mask'].to(device)\n","            decoder_mask = batch['decoder_mask'].to(device)\n","\n","            encoder_output = model.encode(encoder_input, encoder_mask)\n","            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n","            proj_output = model.proj(decoder_output)\n","\n","            label = batch['label'].to(device)\n","            loss = loss_fn(proj_output.view(-1, tokenizer_trgt.get_vocab_size()), label.view(-1))\n","            batch_iterator.set_postfix({'loss': f'{loss.item():6.3f}'})\n","            writer.add_scalar('train loss', loss.item(), global_step)\n","            writer.flush()\n","\n","            loss.backward()\n","\n","            optimizer.step()\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            # if batch_idx % config['checkpoint_every'] == 0:\n","            #     run_validation(model, val_loader, tokenizer_src, tokenizer_trgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n","\n","            global_step += 1\n","\n","        val_loss = run_validation(model, val_loader, tokenizer_src, tokenizer_trgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n","        print(val_loss)\n","\n","        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'global_step': global_step\n","        }, model_filename)"]},{"cell_type":"markdown","metadata":{"id":"e9Gv72hKke86"},"source":["# Running"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"5-qDKqqgs_Lq","outputId":"3e0e6288-7ce8-40fe-ac11-d1935af6ba90","executionInfo":{"status":"error","timestamp":1753523092141,"user_tz":-180,"elapsed":107003,"user":{"displayName":"Владимир Бурцев","userId":"05185084735884685100"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["233\n","232\n"]},{"output_type":"stream","name":"stderr","text":["Processing epoch 00:  11%|█         | 221/1969 [01:37<12:48,  2.27it/s, loss=7.221]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-280-3624594228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-279-1516117117.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(device, config)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mproj_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_trgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mbatch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'{loss.item():6.3f}'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["config = get_config()\n","train_model(device, config)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPqXw+auI1vU0Kbya4A7sDM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}